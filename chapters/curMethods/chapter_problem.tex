\startchapter{Related Work}
\label{chapter:problem}

\section{Misc Notes}

Description of introductory programming at University of Kiel can be found in \cite{Huch05}

Look at ACM/IEEE CS0/CS1/CS2 suggestions...  CC 2001 is at \cite{cc2001} 2008 interim revision is at \cite{cs2008}

Discussion of HtDP and HDMA(?)

Discussion of \cite{SICPbook} and it's influence is described in \cite{Flatt04}.  Famous critique of SICP is in \cite{Wadler87}

\cite{Bos98} mentions that Modula-3 is used as the first year language at the State University of New York at Stony Brook, and also has a rather lengthy discussion of why Java is a poor choice as first language.

\cite{Mahmoud04} has 


``The MIT computer science department in 1997 replaced C++ with Java as the primary software language that students were required to learn'' \cite{Benander04}  It now looks like they are moving to Python.

\cite{Benander04} has great numbers and figures on how various institutions have moved to Java.  Also provides very strong evidence to support the claim that Java is much easier to learn if you already know OO.  Also has support for the claim that Java is easy to learn if you are already experienced as a programmer, thus possibly lend more support to the notion of ``functional first'' (though be careful, it says Java is easy to learn if you know how to program, it doesn't say Java is easy to learn if you know FP).


MIT move from Scheme to Python: http://tech.mit.edu/V125/N65/coursevi.html

\cite{Necaise08} indicates that the popularity of python is the removal of syntax issues -- possible motivation for VPE's?

\cite{Pears07} has an excellent survey of various research questions in regards to the instruction of computer programming, and the design of the first year course.





Objects first?  See 3.4.3.2 of \cite{Pears07} for list of papers referring to Objects first approach

\section{Practices of Teaching Introductory Programming}

\begin{flushright}
\textit{So, if I look into my foggy crystal ball at the future of computing science education, I overwhelmingly see the depressing picture of ``Business as usual''.}
\\
Edsger W. Dijkstra \cite{Dijkstra89} \\
\end{flushright}

In this section we explore how computer programming has been, currently is, and will be taught at various post-secondary institutions.  A thorough survey of literature surrounding how programming has been and is currently being taught can be found in \cite{Pears07}.

\subsection{Curricula}

Introductory programming courses should always be considered within the context of the curriculum in which it lies.  Traditionally, the introductory programming course was one of the first (or was the first) course students undertaking a degree in Computer Science would enroll, as many future Computer Science courses assume basic programming experience as a basic skill.

In North America, many introductory computing courses follow the guidelines outlined in the Computing Curricula recommendations by the ACM and IEEE Computer Society Joint Task Force.  The latest full curriculum recommendation came in 2001 (known as CC2001), and can be found in \cite{cc2001}.  In 2008 an interim revision from the task force was submitted and is at \cite{cs2008}.  CC2001 identified 14 areas which comprised the body of knowledge for computer science at the undergraduate level.  These are listed in \tref{tab:CC2001BOK}.

\begin{table}
	\caption{The 14 Areas Which Comprise the CC2001 Body Of Knowledge For Computer Science}
\begin{tabular}{l}
Discrete Structures (DS) \\
Programming Fundamentals (PF) \\
Algorithms and Complexity (AL) \\
Architecture and Organization (AR) \\
Operating Systems (OS) \\
Net-Centric Computing (NC) \\
Programming Languages (PL) \\
Human-Computer Interaction (HC) \\
Graphics and Visual Computing (GV) \\
Intelligent Systems (IS) \\
Information Management (IM) \\
Social and Professional Issues (SP) \\
Software Engineering (SE) \\
Computational Science and Numerical Methods (CN) \\
\end{tabular}
	\label{tab:CC2001BOK}
\end{table}

Each of these areas are subdivided into units, and each unit consists of a number of topics.  Of the various units, some are given additional \emph{weight} in the recommendation by being designated as \emph{core} units, which are intended as being fundamental to \textbf{any} student of computer science irregardless of area of emphasis.  All units which are not designated as core units are referred to as \emph{elective} units.  While the core units comprise a fundamental set of topics for computer scientists, by themselves the core units (and the topics within) do not comprise a \emph{complete} set of material for a computer scientist -- they need to be supplemented with other elective units from other areas depending on the area of emphasis, the needs and goals of the education institution, etc.

For many years, this task force proposed a two-course introductory programming sequence commonly generically referred to as the CS1/CS2 sequence.  Most educational institutions in North America have followed this pattern, though the contents of the courses have evolved over time.  Roughly speaking, traditionally CS1 tends to focus on introductory programming concepts (control flow and conditional statements, variables, iteration, etc), and CS2 tends to focus more on the introduction of data structures (trees, queues, etc).  In CC2001, the importance of the Object-orientated paradigm was recognized, and it was suggested that object-orientated concepts become a central part of the introductory course sequence.  Partly as a result of this addition, CC2001 also suggested a third course to follow CS2 (typically called CS3) to ensure ``students are able to master these fundamental concepts before moving on''\cite{cc2001}.  Adoption of the three course model has been mixed, with many institutions retaining the two course sequence.

\subsection{Pedagogy}

``While the curriculum defines what is to be taught, pedagogy deals with the manner in which teaching and learning are managed in order to facilitate desired learning outcomes''



Objects-first vs Imperative/Programming first vs functional first.  Contrast with design-first.  Difficulty of OO early.  FP early discussion.

Support that OO first is controversial is in \cite{Astrachan05}.  More heavy criticism of OO first is in \cite{Hu04} Excellent summary of pros and cons of OO first is in \cite{Lister06}

There has been a move toward a ``design-first'' instructional style for the introductory course.  Traditionally introductory programming courses are taught by example.  The instructor introduces a new syntactic construct from the language being used, shows a number of examples using this construct, then exercises or assignments are given where students have to take the given code from the instructor and modify it to a new problem.  Some people feel that this approach emphasizes the language more than design, and given that the specific language in use is not the primary goal of the course, it has been argued that this creates courses whereby students walk away feeling as though programming is all about learning syntax, and not about designing creative solutions to interesting problems.  The consequence of this approach is that it makes the teaching of syntax explicit and the teaching of design implicit, potentially causing courses to create students who can take existing code in Java or C++ (whichever language is used) and modify it to a new problem, but cannot design a solution to a problem from scratch.  \cite{Flatt04} outlines a course whereby design is made much more explicit to students, and has reported success in their approach.

There has also been research done which would indicate that perhaps beginning in the object-orientated paradigm is a less desirable choice than beginning in the functional paradigm\cite{Flatt04,Huch05}.  This has significant repercussions in terms of language choice, as (for example) Java is a heavily object-orientated language, and as such it has been argued is not the ideal choice for a first year course\cite{Bos98,Huch05}.  The TeachScheme/ReachJava project shows that ``functional first'' does not imply that object-orientation must be avoided in the first year of study \cite{Bloch08,teachScheme,Felleisen04}.


\subsection{Language Choice}

\begin{flushright}
\textit{The limits of my language mean the limits of my world.}
\\
Ludwig Wittgenstein \cite{Wittgenstein22} \\
\end{flushright}

The choice of programming language is a critical one, though perhaps an overstated one.  Ideally the introductory programming course should be a course which teaches systematic thinking and problem solving, not ``how to write Java''.  Thus, while by necessity the introductory programming courses must make use of a programming language and environment, the primary goal of the introductory course is to teach programming in the abstract sense rather than specific syntax.

In the history of the field, computer science has seen a variety of languages employed at one time or another as being ``the teaching language of choice''.  For our purposes we shall outline three time-frames and list some of the languages most commonly in use during those times as well as summarize findings in regards to the successes and failures of those languages from a pedagogical standpoint.

\subsubsection{The Past}

FORTRAN, Scheme, Pascal

\subsubsection{Present Day}

C/C++, Java

Problems with Java: \cite{Bos98,Benander04}

\subsubsection{The Future}

As the result of the difficulties associated with the use of C/C++ and Java in the introductory programming course, many institutions are now switching from C/C++ and/or Java to other languages.  The most common of these being the Python programming language \cite{python}.  Python has the advantage of being a scripted, interpreted language, thus students can ``try-out'' small expressions or snippets of code without having to get a complete source file free from errors.  It also has a relatively ``clean'' and simple syntax compared to the relatively verbose Java.  The use of whitespace for program structure encourages good indentation habits early as well.  With all these benefits it is not surprising to see some institutions beginning to adopt Python as the language for their introductory programming courses.  Perhaps the most notable adoption of Python happened at MIT, whose Computer Science department had long been a strong proponent of the use of Scheme in introductory programming courses due to the adoption of \cite{SICPbook} as the introductory text \cite{Thetech06}.  Other examples of using Python in CS1/CS2 style courses and the possible benefits and drawbacks of doing so can be found in \cite{Radenski06,Shannon03,Agarwal05,Agarwal08}.  While there have been successes with Python in the classroom, not even the strongest supporters of the language claim that it is the ``perfect choice''.

\section{Visual Programming Environments}

Alice, Scratch, Gamemaker, VFPE, Prograph

\cite{Hundhausen09} shows some support for the claim that a visual environment (or direct manipulation as he puts it) can provide positive ``transfer of training'' to a textual environment.  Draws a distinction between making programming easier to learn in a general sense and being able to transfer knowledge and understanding to a textual environment.

Use of Game Maker in educational setting at UVic can be found in \cite{Gooch08}.


\section{Games and Computer Science Curricula}
\label{chapter:problemSec:games}

In recent years there has been a notable decline in enrollment in Computer Science courses \cite{Manaris07,Vesgo07,Ward08,Bayliss09}.  As a result of this decline, many institutions are seeking out interesting and new ways to ``entice'' students to enroll in Computer Science courses and to rekindle interest in the discipline.  One such method that has been proposed is to incorporate computer games into Computer Science curricula.  \cite{Cliburn06} provides a summary of how games have been used in this fashion.

The rationale behind this move is that since video games are an exciting and compelling application of computer programming, that perhaps that interest in games can be leveraged by instructors in their introductory programming courses and beyond \cite{Overmars04,Sweedyk05,Barnes08}.  Furthermore, video games are a multi-billion dollar industry \cite{Wallace06}, and a common motivation for students in choosing a discipline are the employment opportunities a degree in the field would entail.  Since electronic entertainment is so prosperous, having curricula that targets students specifically for this field can be a compelling factor in one choosing a degree in Computer Science.

Some institutions have even taken this move a step further and are now offering gaming themes and concentrations in their degrees.  Recently, the University of Victoria added a ``Graphics and Gaming'' option to their Computer Science major program.  Other attempts at this are explored in \cite{Leutenegger07,Murray06,Zyda06}.  Most institutions that incorporate games into their courses or degrees have reported an increase in student enjoyment.  It has been shown that students prefer assignments based around games than ``traditional'' or ``story-telling'' assignments \cite{Cliburn08}.

%\cite{Cliburn08} provides strong evidence that in a general, aggregate study, students prefer assignments based around games than ``traditional'' or ``story-telling'' assignments.

While there have been successes reported surrounding the use of games, it is very much a controversial choice, as there are a number of concerns that have been raised in regards to gaming-centered curricula.  There is strong evidence to indicate that while gaming may spark initial interest in computing, it does not follow that this initial interest will translate into increases in students undertaking Computer Science majors.  A survey of 1,872 students conducted at a ``highly selective public technical university'' found that while 43\% of students indicated that games influenced their interest in computing, only 6.9\% realized that interest by becoming Computer Scientists \cite{DiSalvo09}.  Furthermore, while student interest generally seems to increase with assignments making use of games, few institutions have reported any noticible improvement in student performance.  Cliburn reported findings of a study he performed in the introductory programming course at Hanover College and found that students had a higher overall average score (95.1\% on average) on traditional assignments than on game-based assignments (89.1\% on average)\cite{Cliburn06}.  Interestingly, he also found that in spite of this, most students (78.9\%) still preferred game-based assignments over ``traditional'' assignments, perhaps a further testament to the motivating power of games as assignments.

Concerns have also been raised surrounding issues of gender and race.  One such concern suggests that games appeal more to male students than female, and that incorporation of games may alienate females from the discipline.  Specifically, there has been evidence to show that females tend to prefer games which are cooperative in nature rather than competitive, and that the latter can deter interest of women in games \cite{Camp02}.  This would indicate that educators must be careful about how they incorporate games into courses, and to design course materials with this concern in mind \cite{Carmichael08}.  Other works have adopted ``story-telling'' rather than games as being the vehicle of motivation, and have specifically explored this avenue with middle-school girls with great success \cite{Kelleher06}.

\begin{comment}
\cite{Natvig04,Barnes08} example of games being used
Some ideas about why creativity is important can be found in \cite{Farooq06}.
Games as a motivational tool: see ``International Conference on Game Development in Computer Science Education''.
An excellent outline of how games can be used successfully in CS curricula as well as how not only should we shoehorn games but also focus on fundamentals is summarized in \cite{Bayliss09}.  It also discusses what considerations should be made when incorporating games into courses.
\end{comment}

\section{The Cognitive Dimensions Framework}
\label{chapter:problemSec:cgframework}

One of the challenges faced in evaluating a programming environment is that programming is such a \textit{subjective}
task.  Different programmers have different preferences, and value certain features over others.  While one
programmer may find one aspect of the environment very useful, another may find that aspect counterintuitive.  
Programmers are famous for being extremely opinionated about the tools and environments they use, developing almost
dogmatic devotion to one approach at the expense of others.  It can be difficult to find a formal method to
use to discuss the merits of one language design over another that can be agreed upon.

Oftentimes when computer scientists and software engineers try to evaluate the usability of a system, they turn
to human computer interaction (HCI) principles to do so.  However, as pointed out in \cite{green96}, this is 
problematic for the evaluation of programming environments as typically HCI tends to focus on interactive situations
rather than notational design.  That is, HCI tends to focus more on ``micro-tasks'' and the finished product, rather
than the processes and activities which produce that task.  As an example, in evaluating a programming environment it is less important
to know that the ``compile'' option is easy to find than it is to know that the environment does not allow for new 
abstractions to be created.  Moreover, oftentimes programmers are not HCI experts, and
vice-versa.  Thus while the vocabulary of HCI may be second nature to those in the field, it is less so for the
average computer programmer or software engineer.  Similarly, it is not uncommon for programmers to use terminology
that is unfamiliar to those who are not software developers.  For our purposes, we shall use the ``cognitive dimensions 
framework'' outlined by Green in \cite{green96} for evaluating differing visual programming environments.  

In this section we will outline the cognitive dimensions framework, giving an overview of the various dimensions of the framework, as well as recap some of the ways in which it has been applied to other environments.

\subsection{Outline of the Cognitive Dimensions Framework}
\label{cgframeoutline}

The Cognitive Dimensions Framework consists of 13 different ``dimensions'' to easily evaluate and critically examine a programming
environment.  The purpose of the framework is to provide a set of 
dimensions which capture much of the psychology and HCI of programming, and give a vocabulary for which VPL
designers and users can use to examine different visual programming environments.  It is less of a framework
for stating that ``language X is better than language Y'', but more for statements like ``language X can have
better Y by changing Z'' (where Y is one dimension of the framework, and Z is some aspect of the VPE that
impacts Y).  Furthermore, the language used in the framework is more accessible to the average software developer, 
not requiring one to be an HCI expert to use it.

One very important aspect of the framework is that (not surprisingly) different dimensions can represent trade-offs.  In fact, 
this is intentional --- there are always trade-offs in designing any system of moderate complexity, and until there is a 
vocabulary for discussing those trade-offs it is difficult to reason about how one can improve the initial design.  The cognitive 
dimensions framework is intended as being able to fill this void, allowing designers to coherently and critically examine 
and \textit{converse} about their designs so that they may find the best compromise given the objective(s) of the system 
being designed.  As Green himself puts it ``Like other forms of engineering, design is a matter of compromise''. \cite{green96}

Below is a recap of the listing of the thirteen dimensions of the Cognitive Dimensions Framework described by Green in \cite{green96}. 

\begin{itemize}
	\item Abstraction Gradient - What are the minimum and maximum levels of abstraction?
	\item Closeness of Mapping - What programming ``games''	need to be learned?
	\item Consistency - When some of the language has been learned, how much of the remaining parts of the language can be inferred?
	\item Diffuseness - How many symbols or graphic entities are required to express a meaning?
	\item Error-proneness - Does the design of the notation induce ``careless mistakes''?
	\item Hard Mental Operations - Are there places where the user needs to resort to fingers or penciled annotation to keep track of what's happening?
	\item Hidden Dependencies - Is every dependency overtly indicated in both directions? Is the indication perceptual or only symbolic?
	\item Premature Commitment - Do programmers have to make decisions before they have the information they need?
	\item Progressive Evaluation - Can a partially-complete program be executed to obtain feedback on ``How am I doing?''
	\item Role-expressiveness - Can the reader see how each component of a program relates to the whole?
	\item Secondary notation - Can programmers use layout, colour, or other cues to convey extra meaning, above and beyond the 'official' semantics of the language?
	\item Viscosity - How much effort is required to perform a single change?
	\item Visibility - Is every part of the code simultaneously visible (assuming a large enough display), or is it possible to juxtapose any two parts side-by-side at will?
\end{itemize}

A more detailed description of each is given in the sections that follow.

\subsubsection{Abstraction Gradient}
\label{absgradientoutline}

\cite{green96} describes the Abstraction Gradient as ``What are the minimum and maximum levels of abstraction?  Can 
fragments be encapsulated?''  Furthermore, Green classifies languages based upon three categories: abstraction-hating, 
abstraction-tolerant, or abstraction-hungry depending upon the languages minimum starting level of abstraction and their 
readiness or desire to accept further abstraction.

That is, this dimension asks the questions: how much needs to be constructed and learned in order to begin making the programming
environment perform a task?  And how easy is it to add new abstractions?  

The example Green gives of an abstraction-hating formalism is that of flowcharts, as they only allow decision boxes 
and action boxes.  There is no way to ``add'' additional constructs or group related ideas within a flowchart.  While they 
have a low minimum level of abstraction (there is not much to learn to begin using them), they have no readiness to accept
further abstractions.  Turing 
machines would be another example of a notation which is abstraction hating --- each instruction can only have a current 
state, current symbol, next state, next symbol, and a direction to move on the tape.

The classic example of an abstraction-tolerant language would be C, as it has a higher minimum level of abstraction (one must
learn some of the various keywords in the language, as well as how to write a \code{main()} routine for example), and it allows
for new abstractions of various kinds to be created (for example, one can create new types with \code{typedef} and \code{struct}).

The example Green gives of an abstraction-hungry language is Smalltalk, as it has a high starting level of abstraction, 
and requires one to modify the class hierarchy in order to create new programs.  That is, every program written in the
language \textbf{requires} the introduction of new abstractions.

From the perspective of learning to program, it has been shown that oftentimes
students new to computer programming struggle with languages which force one to learn and master several 
abstractions at once, giving them ``a `rubber hamburger' which has to be swallowed because it cannot be chewed''\cite{green96}.
As such it is difficult to find the right balance here, requiring too many abstractions to be learned can negatively
impact learning, but having limited abstraction mechanisms can make programs difficult to modify (see the section on Viscosity
later).  Green seems to indicate that abstraction-tolerant languages are the best fit for learning environments, but admits
that more research needs to be done in this area.

\subsubsection{Closeness of Mapping}
\label{closenessoutline}

One could argue that programming is all about problem solving.  Put another way, given a problem expressed in one particular domain (the problem domain) the role of the programmer is to come up with a mapping of this problem into the domain of the programming environment in use (the program domain).  Closeness of mapping tries to measure the gap between the problem domain and the program domain.  That is, how much extra ``administrative stuff'' does the environment impose upon the programmer in order to translate a solution to a problem into something that can be executed in the programming environment?  Green refers to this ``administrative stuff'' as ``programming games'', or the little language/environment-specific idiosyncrasies that are imposed upon the programmer.  The classic example of a programming game which is imposed on many first year computer science students is the \code{public static void main (String [] args)} method signature that must appear in any Java program.  This line is (typically) completely absent from the problem domain, yet is a required artifact that students must produce in order to produce a solution to any problem given to them by their instructors when Java is the language of choice.

Closeness of Mapping also is to a lesser degree a measure of what the environment provides in terms of standard libraries and constructs.  If a language provides a great deal of standard libraries, then intuitively it would seem that there would be less work for the programmer to do to create the mapping from problem domain to program domain.  Related to this is the notion of whether or not the language allows constructs to be built that improve the Closeness of Mapping.  That is, if functionality is not provided in the way of standard libraries, is it possible for one to add their own to better bridge the gap between
problem and program domain.

Traditionally (and not surprisingly), domain-specific languages tend to score very well in this regard \emph{when the problem lies within the target domain that the language was designed for}.  It is less clear as to how well these languages score when attempting general problems that fall outside of that targeted domain.

\subsubsection{Consistency}
\label{consistencyoutline}

Consistency tries to measure how easy it is to infer the remaining parts of the language once one has learned part of the language.  Put another way, consistency assesses whether or not ``similar semantics are expressed in similar syntactic forms''.\cite{green98}  Green states consistency as a form of ``guessability'': ``when a person knows some of the language structure, how much of the rest can be successfully guessed?''\cite{green96}  An example that has been given for a way in which a language suffers from consistency problems is Pascal and its handling of reals and integers versus boolean variables.  In Pascal, one can read and write to reals and integers, but boolean variables may only be read from even though syntactically booleans are essentially used and treated in the same way as other variables.

Another more modern example would be Java and its String data type.  Strictly speaking, \code{String}s are object types in Java, but can be treated in much the same way as primitive data types (such as \code{int}s or \code{boolean}s).  For example, one can use the concatenation operator to combine two strings together in infix notation as seen in \pref{prog:javainfix}.  This is the only time in which object types may be used in this ``operator overloaded'' way, all other object types must resort to method calls (as seen in \pref{prog:javamethod}).  ``Special cases'' such as this can greatly hurt the consistency of a language and make them harder to learn.

\begin{program}
\begin{verbatim}
String s1 = "Hello";
String s2 = " World";
String s3 = s1 + s2;  // create "Hello World" 
\end{verbatim}
\caption{Concatenating Two Java Strings Using an Operator in Infix Notation}
\label{prog:javainfix}
\end{program}

\begin{program}
\begin{verbatim}
String s1 = "Hello";
String s2 = " World";
String s3 = s1.concat (s2); // create "Hello World" 
\end{verbatim}
\caption{Concatenating Two Java Strings Using Method Calls}
\label{prog:javamethod}
\end{program}

Typically visual languages tend to score well in consistency, largely due to a simpler syntax.  Many textual-based languages have struggled in regards to consistency as most ``real-world'' textual languages have grammars of substantial size, and generally speaking the larger the grammar, the more complex the syntax (and thus the more likely there will be inconsistencies that arise).
 
Additionally, as noted by \cite{Kelso02} an issue related to consistency is that of library regularity.  For example, Haskell generally keeps naming and argument ordering in libraries in regular order (thus scoring well in terms of consistency).  C libraries however tend to not score so well, largely due to the fact that they have been developed over a long period of time by a large number of authors\footnote{As well, it could be argued that the fact that all functions in C reside in the same namespace negatively impacts consistency as well as it means that library designers must come up with their own unique naming schemes for functions to avoid naming conflicts.}.

It has also been noted that consistency can be viewed from two different perspectives: the learner and the designer. \cite{Reisner93} Ideally these two perspectives should be the same, but in practice they often are not.  The distinction in Java between object types and primitive types is an example of this.  From the designers perspective this is perfectly consistent -- there are two fundamental categories of data types, and within the Java Virtual Machine (JVM) they are handled completely differently and separately.  From a learner's perspective however, this distinction can create confusion.  From a learner (or user of the language) it is all just data so the distinction is not as apparent and, for many new to the language, seems rather arbitrary.

At any rate, given that the Cognitive Dimensions framework was designed by Green to be used by designers of a language one might think that his intention was that consistency was to be applied from the designers perspective, however this is not explicitly stated.  From our perspective as evaluators of a VPE from a pedagogical standpoint we are however more interested in the learners perspective, and our application of this dimension in evaluating the Gem Cutter will reflect this.

\subsubsection{Diffuseness}
\label{diffusenessoutline}

Diffuseness is also sometimes referred to as terseness.  That is, it tries to measure how many symbols (or graphic entities in the case of a VPE) are required to express a meaning.  This is, of course, a trade off -- if a language is too terse it can be hard to read, but if a language is too verbose it becomes difficult to ``keep it all in your head'' at once.

Traditionally functional languages have been quite terse, and scored well in that regard\footnote{Some even criticize functional languages for being \emph{too} terse, further illustrating that this dimension represents a trade-off.}.  Visual languages also tend to require a small number of syntactic elements to express a meaning, so one would think that a visual functional programming environment would be the most terse of all environments, and Kelso found this to be true of his VFPE\cite{Kelso02}.

Measuring diffuseness/terseness is done by counting the number of syntactic lexemes that comprise a program.  To give a meaningful comparison, Green uses a sample yardstick problem taken from \cite{curtis89} that he calls the ``rocket trajectory problem'' which is summarized in \fref{fig:rocketTrajectoryProblem}.  He then goes on to give solutions to the problem in each of the environments he looked at, and Kelso did the same for Haskell and his VFPE.  The unfortunate flaw in this approach is that it does not account for individual skill or familiarity with a language -- the more familiar one is with a programming environment, the more likely it will be that one can use the ``right'' constructs and thus produce a solution which is the most terse.  Conversely, if one is unfamiliar with an environment\footnote{And particularly if the environment suffers from poor consistency} one will likely try to use a subset of the full set of constructs available, and end up producing a solution which is overly ``cluttered''.

As well, as noted by Kelso in \cite{Kelso02} it can be difficult to measure diffuseness in visual environments which have dynamic layout schemes.  Oftentimes visual environments allow one to ``collapse'' code blocks into single elements.  Or as Kelso puts it: ``The level of diffuseness can in effect be dynamically traded off against the level of detail''.

\begin{figure}
The rocket program computes the vertical and horizontal trajectory of a rocket on which the only forces acting are its thrust and gravity.  At time zero the rocket stands stationary and vertical on level ground, with a mass of $10^4$ pounds.  Its engine develops a thrust of $4\cdot10^5$ foot-pounds, using up a mass of 50 pounds of fuel per second, until the fuel is exhausted after 100 seconds.  It rises vertically for 10 seconds after which it adopts and retains an angle of 0.3941 radians (22.5 degrees) to the vertical.  The downwards acceleration of gravity is $32 feet/sec^2$.\cite{green96}
  \caption{The Rocket Trajectory Problem}
  \label{fig:rocketTrajectoryProblem}
\end{figure}

\subsubsection{Error-Proneness}
\label{errorpronenessoutline}

The fundamental question that the error-proneness dimension tries to answer is ``does the design of the notation induce `careless mistakes'?''  Green draws a distinction between `mistakes' and `slips', where the former refers to the parts of program design which are deeply difficult irregardless of the notation\footnote{For example, design issues such as how to decompose a larger problem into smaller ones would be an example of a ``deeply difficult'' design problem.}, and the latter refers to those errors that one did not mean to do, where you knew better, but still somehow made a simple little ``slip-up''.  Error-proneness tries to measure whether or not the notation itself oftentimes leads to or encourages these ``slips''.

An example from the world of textual languages is the use of textual identifiers, particularly in languages which are case-sensitive.  It is very easy to accidentally misspell an identifier in languages which do not require identifiers to be declared before they are used, which can lead to subtle and difficult to debug errors in the program.  The paired-delimiter system (braces in C/C++/Java, parenthesis in languages such as Lisp and Scheme, or the begin/end pair in languages like Pascal) can also lead to trivial errors which cause the code to fail to compile.  Note that all of these are \emph{syntax} errors which are all but avoided in visual programming languages.  Thus, not surprisingly visual environments tend to avoid some of these common ``slips'', however, it would not be fair to say that visual languages are inherently less error-prone than textual languages.

\begin{comment}
Languages such as Pascal which use semicolons as separators rather than statement terminators can cause confusion as well, as often times novices will write: 

\code{if A then B; else C;}

rather than 

\code{if A then B else C;}

The functional language SML has a similar issue in regards to its ``let expression'' syntax.  FIXME - perhaps reference SML reference manual with explanation of how val bindings in lets do not need to end with semicolons.
\end{comment}

Kelso in \cite{Kelso02} also notes that issues related to data types can play a role here.  That is, languages which have strong type checks that are enforced throughout a program will likely be less error-prone than languages which do not enforce such strong and rigorous type checking.

\subsubsection{Hard Mental Operations}
\label{hardmentalopsoutline}

A programmer must already juggle a number of different ideas and concepts while implementing the solution to a task.  What data structure should I use?  How efficient is this algorithm?  Can I access this piece of data within this context?  Ideally we would like our programming environments to reduce the cognitive load imposed upon the programmer.  That is, a programmer's job is hard enough, without having the tools in use add to the amount of things he or she must juggle.  This is where the hard mental operations dimension comes in.  Green defines a hard mental operation as having two properties:

\begin{enumerate}
	\item it must lie at the notational level, rather than the semantic level, as the dimension is trying to measure shortcomings in the programming environment, rather than discover meanings which are inherently difficult to express irregardless of the notation
	\item combining multiple hard mental operations vastly increases the difficulty
\end{enumerate}

As such, this allows him to outline a ``broad-brush'' test for hard mental operations by asking the following two questions: 

\begin{enumerate}
	\item if I fit two or three of these constructs together, does it get incomprehensible?
	\item is there a way in some other language of making it comprehensible? (Or is it just a really hard idea to grasp?)
\end{enumerate}

If the answer to both is yes, then we have a hard mental operation.  As an example application of this, consider the following declaration written in the C programming language:

\code{int *(*(*(*b)())[10])();}

This is taken from \cite{Giguere87}, and it is a declaration which declares a pointer to a function called b which returns a pointer to an array of 10 elements, each of which are pointers to functions which return pointers to ints.

If we examine ``functions as parameters in C'' as a construct, we would ask the question ``if I fit two or three function pointers together, does it get incomprehensible?''  It would appear that the answer to this is yes, a pointer to a function is difficult to mentally parse to begin with, but if we have pointers to functions of pointers to functions, they are even more difficult to decompose.  Next we ask the question ``is there a way in some other language of making function pointers comprehensible?''  Given that higher order functions are essentially functions as parameters, most any functional programming language provide support for this mechanism in a much simpler and easier to understand form.  Thus, it would appear that ``functions as parameters'' is an example of a hard mental operation in the C programming language.

Hard mental operations in particular is an important dimension for environments designed for learning, as presumably students learning a language already have to mentally process the new language or notation that they are learning, in addition to any fundamental ideas or concepts instructors are trying to convey.  Put another way: hard mental operations increase the cognitive load upon students, and it has been shown (FIXME REFERENCE!) that this can have a negative impact upon learning.

\subsubsection{Hidden Dependencies}
\label{hiddendependenciesoutline}

A hidden dependency occurs when there is a relationship between two components such that one is dependent upon the other, and for which this dependency is not fully visible or apparent.  An example from the textual domain that is of particular interest to programmers working within the functional paradigm is that of the side effect.  If we have a subroutine \code{a()} which alters a global variable, this represents a hidden dependency between the variable and the function.  One of the hallmarks of the functional paradigm is the notion of referential transparency, whereby an expression can be replaced with its value without altering the meaning or semantics of the program as a whole, a definition which prohibits these sort of side effects.

Another common example includes the much maligned GOTO statement found in various programming languages.  This is due to the absence of a ``come-from'' which means that the effect of changes to code making heavily use of GOTOs can be difficult to predict.

A much more systemic example of a hidden dependency is the construct that virtually all programming languages make use of -- the named subroutine.  If \code{a()} calls \code{b()} which calls \code{c()}, a change to \code{c()} has implications for both \code{a()} and \code{b()}, though that relationship will likely not be apparent unless one examines the code of those routines.  Call graphs are commonly used to mitigate this concern.

\subsubsection{Premature Commitment}
\label{prematurecommitmentoutline}

The fundamental question to be asked in regards to this dimension is ``do programmers have to make decisions before they have all the information they need?''  Green explains premature commitment is akin to ``writing the contents list -– with page numbers –- before you write the book''\cite{green96}.  Premature commitment occurs when the following three criteria are met:

\begin{itemize}
	\item the notation contains many internal dependencies
	\item the medium/environment constrains the order of doing things
	\item the order is inappropriate
\end{itemize}

For example, consider the C function defined in \pref{prog:hypoCsub}.  In isolation, a source file containing only this code would not compile as there is an internal dependency between the \code{foo()} subroutine, and another routine called \code{bar()}.  The typical C programming environment constrains one from writing subroutines which depend on other subroutines until a definition of those subroutines are given\footnote{The common ``work-around'' for this is the stub routine, where the function prototype and an empty body (or a simple return statement for routines which have return types) is provided by the programmer, and ``filled-in'' later}.  Whether or not this constraint on the order of subroutine creation is appropriate or not shall be left for the reader to decide.

\begin{program}
\begin{verbatim}
int foo ()
{
    return bar() * 42;
} 
\end{verbatim}
\caption{A Hypothetical C subroutine}
\label{prog:hypoCsub}
\end{program}

Furthermore, Green provides a number of categories of premature commitment, outlined as:

\begin{itemize}
	\item Commitment to layout
	\item Commitment to connections
	\item Commitment to order of creation
	\item Commitment to choice of construct
\end{itemize}

\subsubsection{Progressive Evaluation}
\label{progressiveevaluationoutline}

This dimension tries to evaluate to what degree the environment in question allows one to evaluate a partially completed solution.  That is, environments which are strong from the perspective of progressive evaluation will allow evaluating partially complete programs with greater frequency.

As such, traditional text-based, compiled languages tend to score poorly in this dimension -- a Java program cannot be executed until it is compiled, and it cannot be compiled until it is a complete program.  Interpreted environments can do better, as they oftentimes allow one to enter individual expressions and evaluate them at any time\footnote{This is, in fact one of the arguments in favour of using Python as an introductory programming language, as the standard Python interpreter allows one to enter expressions and get immediate results}.  There are still limitations in that the expressions must be ``self-contained'', they cannot contain references to not yet identified values or functions for example.  However, this is more an issue of premature commitment than one of progressive evaluation.

This dimension is of particular importance to students new to programming, as it is recognized that novices need to evaluate their progress frequently to assess whether or not they are appropriately understanding the new concept or idea.

\subsubsection{Role-Expressiveness}
\label{roleexpressivenessoutline}



\subsubsection{Secondary Notation}
\label{secondarynotationoutline}

Talk about Secondary Notation.

\subsubsection{Viscosity}
\label{viscosityoutline}

Talk about Viscosity.

\subsubsection{Visibility}
\label{visibilityoutline}

Talk about Visibility.

\subsection{Application of the Cognitive Dimensions Framework to Other Environments}

FIXME - recap findings in Green and Kelso, and compare with discussion of gem cutter.